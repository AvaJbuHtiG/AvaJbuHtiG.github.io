<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>hadoop之hdfs | zgq's blog</title><meta name="description" content="hadoop之hdfs"><meta name="author" content="zgq"><meta name="copyright" content="zgq"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="twitter:card" content="summary"><meta name="twitter:title" content="hadoop之hdfs"><meta name="twitter:description" content="hadoop之hdfs"><meta name="twitter:image" content="https://avajbuhtig.github.io/img/post.jpg"><meta property="og:type" content="article"><meta property="og:title" content="hadoop之hdfs"><meta property="og:url" content="https://avajbuhtig.github.io/2019/12/29/hadoop%E4%B9%8Bhdfs/"><meta property="og:site_name" content="zgq's blog"><meta property="og:description" content="hadoop之hdfs"><meta property="og:image" content="https://avajbuhtig.github.io/img/post.jpg"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>var autoChangeMode = '1'
var t = Cookies.get("theme")
if (autoChangeMode == '1'){
  var isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
  var isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
  var isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined){
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport){
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour < 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
  }
  } else if (t == 'light') activateLightMode()
  else activateDarkMode()

} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="https://avajbuhtig.github.io/2019/12/29/hadoop%E4%B9%8Bhdfs/"><link rel="prev" title="hadoop之基础集群搭建" href="https://avajbuhtig.github.io/2020/01/01/hadoop%E4%B9%8B%E5%9F%BA%E7%A1%80%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"><link rel="next" title="hadoop基本" href="https://avajbuhtig.github.io/2019/12/05/hadoop%E5%9F%BA%E6%9C%AC/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  highlightShrink: 'false',
  isFontAwesomeV5: false,
  isPhotoFigcaption: false
  
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isSidebar: true  
  }</script><noscript><style>
#page-header {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 4.2.0"></head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">23</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">11</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div></div></div></div><i class="fa fa-arrow-right on" id="toggle-sidebar" aria-hidden="true">     </i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#总述"><span class="toc-number">1.</span> <span class="toc-text">总述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#优缺点"><span class="toc-number">1.1.</span> <span class="toc-text">优缺点</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#无法高效的对大量小文件进行存储"><span class="toc-number">1.1.0.1.</span> <span class="toc-text">无法高效的对大量小文件进行存储</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#不支持并发写入，文件随机读取修改"><span class="toc-number">1.1.0.2.</span> <span class="toc-text">不支持并发写入，文件随机读取修改</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hdfs架构组成"><span class="toc-number">1.2.</span> <span class="toc-text">hdfs架构组成</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#块大小"><span class="toc-number">1.3.</span> <span class="toc-text">块大小</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#hadoop的操作"><span class="toc-number">2.</span> <span class="toc-text">hadoop的操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#shell操作"><span class="toc-number">2.1.</span> <span class="toc-text">shell操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#客户端操作"><span class="toc-number">2.2.</span> <span class="toc-text">客户端操作</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#hdfs的工作流程"><span class="toc-number">3.</span> <span class="toc-text">hdfs的工作流程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#nameNode和SecondNameNode工作原理"><span class="toc-number">3.1.</span> <span class="toc-text">nameNode和SecondNameNode工作原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#fsImage和Edits解析"><span class="toc-number">3.2.</span> <span class="toc-text">fsImage和Edits解析</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#ovi命令和oev命令"><span class="toc-number">3.2.1.</span> <span class="toc-text">ovi命令和oev命令</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#checkPoint时间设置："><span class="toc-number">3.3.</span> <span class="toc-text">checkPoint时间设置：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#nameNode故障处理"><span class="toc-number">3.4.</span> <span class="toc-text">nameNode故障处理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#集群安全模式"><span class="toc-number">4.</span> <span class="toc-text">集群安全模式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DataNode"><span class="toc-number">5.</span> <span class="toc-text">DataNode</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#工作机制"><span class="toc-number">5.1.</span> <span class="toc-text">工作机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#nameNode掉线时间参数设置"><span class="toc-number">5.2.</span> <span class="toc-text">nameNode掉线时间参数设置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#新增节点和退役节点"><span class="toc-number">5.3.</span> <span class="toc-text">新增节点和退役节点</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#新增节点"><span class="toc-number">5.3.1.</span> <span class="toc-text">新增节点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#退役节点："><span class="toc-number">5.3.2.</span> <span class="toc-text">退役节点：</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#白名单方案："><span class="toc-number">5.3.2.1.</span> <span class="toc-text">白名单方案：</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#黑名单方案"><span class="toc-number">5.3.2.2.</span> <span class="toc-text">黑名单方案</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#dataNode多目录设置"><span class="toc-number">5.3.3.</span> <span class="toc-text">dataNode多目录设置</span></a></li></ol></li></ol></li></ol></div></div></div><div id="body-wrap"><div class="post-bg" id="nav" style="background-image: url(/img/post.jpg)"><div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">zgq's blog</a></span><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span></span></div><div id="post-info"><div id="post-title"><div class="posttitle">hadoop之hdfs</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="发表于 2019-12-29 21:03:53"><i class="fa fa-calendar" aria-hidden="true"></i> 发表于 2019-12-29</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="更新于 2020-04-22 20:45:47"><i class="fa fa-history" aria-hidden="true"></i> 更新于 2020-04-22</span></time></div><div class="meta-secondline"> </div><div class="meta-thirdline"><span class="post-meta-pv-cv"><i class="fa fa-eye post-meta__icon" aria-hidden="true"> </i><span>阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-commentcount"></span></div></div></div></div><main class="layout_post" id="content-inner"><article id="post"><div id="article-container"><a id="more"></a>

<!-- hadoop四大组价之一 hdfs (2.0) -->

<p>这里只是简单记录一些问题，刚开始学习，后期补充。</p>
<h2 id="总述"><a href="#总述" class="headerlink" title="总述"></a>总述</h2><h3 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h3><p>优点不多说，容错性高，适合处理大数据。</p>
<p>缺点：</p>
<p>不适合低延迟的数据访问，无法高效的对大量小文件进行存储，不支持并发写入，文件随机读取修改。</p>
<h5 id="无法高效的对大量小文件进行存储"><a href="#无法高效的对大量小文件进行存储" class="headerlink" title="无法高效的对大量小文件进行存储"></a>无法高效的对大量小文件进行存储</h5><p>这里主要有2点原因：1、大量的小文件会占用nameNode大量的内存来存储文件的元数据信息。2、小文件存储寻址时间会超过读取时间，违反了HDFS的设计目标。</p>
<p>hdfs的设计目标中，寻址时间应大约为传输时间的1%100，则为最佳。</p>
<h5 id="不支持并发写入，文件随机读取修改"><a href="#不支持并发写入，文件随机读取修改" class="headerlink" title="不支持并发写入，文件随机读取修改"></a>不支持并发写入，文件随机读取修改</h5><p>hdfs中单个文件只支持一个线程进行写操作，不允许多个线程同时写，并且hdfs中的文件只支持数据append，不支持随机修改。</p>
<h3 id="hdfs架构组成"><a href="#hdfs架构组成" class="headerlink" title="hdfs架构组成"></a>hdfs架构组成</h3><p>在hadoop的基础介绍里面就讲过了，hdfs由nameNode，dataNode，SecondNameNode组成。</p>
<p><img src="/" class="lazyload" data-src="C:%5CUsers%5Czgq%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1577709282091.png"  alt="1577709282091"></p>
<p><img src="/" class="lazyload" data-src="C:%5CUsers%5Czgq%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1577709376222.png"  alt="1577709376222"></p>
<p>但是这里的SNN可会帮助恢复NN有一定的问题，会有一定的数据丢失问题。后面讲到NameNode的工作原理会解释。</p>
<h3 id="块大小"><a href="#块大小" class="headerlink" title="块大小"></a>块大小</h3><p>在hdfs中，存储单位为文件块。2.x中，块大小为128兆，老版本中为64兆。</p>
<p>在前面说过了，寻址时间约为传输时间的1%100，则为最佳。</p>
<p>根据实际情况来说，块大小不是固定不变的。不能太大，也不能太小，需要根据磁盘的传输速率开决定。块设置太小，读取一个文件寻址的次数就会增加，寻址时间会大大增加。</p>
<p>注：在hdfs中，读取一个文件寻址的次数取决于该文件分了几个文件块存储，hdfs只要找到每个文件快的起始地址，根据块大小就可以确定结束地址。</p>
<p>文件块设置太大的话，从磁盘传输的时间会明显大于该块的寻址时间，导致程序在处理这块数据时，会非常慢。</p>
<h2 id="hadoop的操作"><a href="#hadoop的操作" class="headerlink" title="hadoop的操作"></a>hadoop的操作</h2><h3 id="shell操作"><a href="#shell操作" class="headerlink" title="shell操作"></a>shell操作</h3><p>主要命令：</p>
<p>hadoop fs xxxx 或者 hdfs dfs xxx   dfs 是fs的实现类</p>
<p>在linux的shell中适用的命令在hadoop中同样适用。比如 ls,mkdir,cat,copy,mv,tail等。唯一不同的是，需要加</p>
<p>hadoop fs -xx 或hdfs dfs -xx.。有几个命令没见过，这里写下 copyFromLocal(put),copyToLocal(get),</p>
<p>getmerge: 合并下载多个文件。</p>
<p>rmdir:删除空文件夹</p>
<p>setrep：设置文件副本数量。这里设置的副本数只是记录在NameNode的元数据中，是否真的会有这么多副本，还得看DataNode的数量。因为目前只有3台设备，最多也就3个副本，只有节点数的增加到10台时，副本数才能达到10</p>
<h3 id="客户端操作"><a href="#客户端操作" class="headerlink" title="客户端操作"></a>客户端操作</h3><p>hdfs的客户端操作需要引入一些hadoop相关的依赖。</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependencies&gt;</span><br><span class="line">		&lt;dependency&gt;</span><br><span class="line">			&lt;groupId&gt;junit&lt;/groupId&gt;</span><br><span class="line">			&lt;artifactId&gt;junit&lt;/artifactId&gt;</span><br><span class="line">			&lt;version&gt;RELEASE&lt;/version&gt;</span><br><span class="line">		&lt;/dependency&gt;</span><br><span class="line">		&lt;dependency&gt;</span><br><span class="line">			&lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;</span><br><span class="line">			&lt;artifactId&gt;log4j-core&lt;/artifactId&gt;</span><br><span class="line">			&lt;version&gt;2.8.2&lt;/version&gt;</span><br><span class="line">		&lt;/dependency&gt;</span><br><span class="line">		&lt;dependency&gt;</span><br><span class="line">			&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">			&lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;</span><br><span class="line">			&lt;version&gt;2.7.2&lt;/version&gt;</span><br><span class="line">		&lt;/dependency&gt;</span><br><span class="line">		&lt;dependency&gt;</span><br><span class="line">			&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">			&lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;</span><br><span class="line">			&lt;version&gt;2.7.2&lt;/version&gt;</span><br><span class="line">		&lt;/dependency&gt;</span><br><span class="line">		&lt;dependency&gt;</span><br><span class="line">			&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">			&lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;</span><br><span class="line">			&lt;version&gt;2.7.2&lt;/version&gt;</span><br><span class="line">		&lt;/dependency&gt;</span><br><span class="line">		&lt;dependency&gt;</span><br><span class="line">			&lt;groupId&gt;jdk.tools&lt;/groupId&gt;</span><br><span class="line">			&lt;artifactId&gt;jdk.tools&lt;/artifactId&gt;</span><br><span class="line">			&lt;version&gt;1.8&lt;/version&gt;</span><br><span class="line">			&lt;scope&gt;system&lt;/scope&gt;</span><br><span class="line">			&lt;systemPath&gt;$&#123;JAVA_HOME&#125;/lib/tools.jar&lt;/systemPath&gt;</span><br><span class="line">		&lt;/dependency&gt;</span><br><span class="line">&lt;/dependencies&gt;</span><br></pre></td></tr></table></figure>

<p>这里只要是介绍下上面提到的hadoop的命令，客户端的实现。</p>
<p>hadoop客户端把hdfs抽象成一个文件系统。hdfs的一些相关配置信息通过Configuration在对象中设置，然后传入到抽象出的文件系统中。</p>
<p>简单的命令不介绍，介绍两个相似命令的区别：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@RunWith</span>(SpringRunner<span class="class">.<span class="keyword">class</span>)</span></span><br><span class="line"><span class="class">@<span class="title">Slf4j</span></span></span><br><span class="line"><span class="class">@<span class="title">SpringBootTest</span></span></span><br><span class="line"><span class="class"><span class="title">public</span> <span class="title">class</span> <span class="title">HdfsServiceApplicationTests</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> FileSystem fs;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Before</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">prefixDo</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        Configuration config = <span class="keyword">new</span> Configuration();</span><br><span class="line">        fs = FileSystem.get(URI.create(<span class="string">"hdfs://192.168.40.139:9000"</span>), config, <span class="string">"mpaas"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@After</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span>  <span class="title">after</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        fs.close();</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 只查询文件，不会包含文件夹,通过是否递归的参数，可以选择性的查询指定目录下文件夹里面的文件</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@author</span> GuoQiang.Zhou</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@date</span> 2019/12/22 19:04</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span></span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">listFiles</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        RemoteIterator&lt;LocatedFileStatus&gt; fileIterator = fs.listFiles(<span class="keyword">new</span> Path(<span class="string">"/dfs"</span>), <span class="keyword">true</span>);</span><br><span class="line">        <span class="keyword">while</span> (fileIterator.hasNext()) &#123;</span><br><span class="line">            LocatedFileStatus fileStatus = fileIterator.next();</span><br><span class="line">            BlockLocation[] blockLocations = fileStatus.getBlockLocations();</span><br><span class="line">            <span class="keyword">for</span> (BlockLocation location : blockLocations) &#123;</span><br><span class="line">                log.info(<span class="string">"文件块位于&#123;&#125;"</span>, location.getNames());</span><br><span class="line">            &#125;</span><br><span class="line">            log.info(<span class="string">"&#123;&#125;"</span>, fileStatus.getOwner());</span><br><span class="line">            log.info(<span class="string">"&#123;&#125;"</span>, fileStatus.getPermission());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     *  查询文件和文件夹，不能递归查询，只能查到指定路径下的文件和文件夹</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@author</span> GuoQiang.Zhou</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@date</span> 2019/12/22 19:05</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span></span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">listStatus</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        FileStatus[] fileStatuses = fs.listStatus(<span class="keyword">new</span> Path(<span class="string">"/dfs"</span>));</span><br><span class="line">        <span class="keyword">for</span> (FileStatus fileStatus : fileStatuses) &#123;</span><br><span class="line">            log.info(<span class="string">"&#123;&#125;"</span>, fileStatus.getPath());</span><br><span class="line">            <span class="comment">// 判断是否文件夹，还有 isEncrypted, isSymlink, isFile,</span></span><br><span class="line">            <span class="keyword">if</span> (fileStatus.isDirectory()) &#123;</span><br><span class="line">                FileStatus[] innerFileStatuses = fs.listStatus(fileStatus.getPath());</span><br><span class="line">                <span class="keyword">for</span> (FileStatus status : innerFileStatuses) &#123;</span><br><span class="line">                    log.info(<span class="string">"&#123;&#125;文件夹内文件&#123;&#125;"</span>, fileStatus.getPath(), status.getPath());</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这里涉及到一个问题。如果文件比较大，被分成了几个文件块，可以分别下载每个文件块，(快信息可以通过上面的命令查询到)，然后用命令把块合并成最终的文件。</p>
<p>win下面命令如下，linux还没试过，后面补充  // TODO</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">type hadoop-2.7.2.tar.gz.part2 &gt;&gt; hadoop-2.7.2.tar.gz.part1</span><br></pre></td></tr></table></figure>



<h2 id="hdfs的工作流程"><a href="#hdfs的工作流程" class="headerlink" title="hdfs的工作流程"></a>hdfs的工作流程</h2><p>文件写入流程大致如下</p>
<p><img src="/" class="lazyload" data-src="C:%5CUsers%5Czgq%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1577713381493.png"  alt="1577713381493"></p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1）客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在。</span><br><span class="line">2）NameNode返回是否可以上传。</span><br><span class="line">3）客户端请求第一个 Block上传到哪几个DataNode服务器上。</span><br><span class="line">4）NameNode返回3个DataNode节点，分别为dn1、dn2、dn3。</span><br><span class="line">5）客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。</span><br><span class="line">6）dn1、dn2、dn3逐级应答客户端。</span><br><span class="line">7）客户端开始往dn1上传第一个Block（先从磁盘读取数据放到一个本地内存缓存），以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答。</span><br><span class="line">8）当一个Block传输完成之后，客户端再次请求NameNode上传第二个Block的服务器。（重复执行3-7步）</span><br></pre></td></tr></table></figure>

<p>这里涉及到一个问题，在第4步中，nameNode返回给客户端的节点，是如何计算出来的。</p>
<p>NameNode会选择距离待上传数据最近距离的DataNode接收数据。那么这个最近距离怎么计算呢。</p>
<p>这个最近距离指的是网络拓扑结构中的最近距离。即网络结构构成的有向图中的最近距离。</p>
<p>然后第二个节点，第三个节点又是如何选择的呢(副本数为3的前提下)，这里就要知道hadoop的机架感知了。</p>
<p>如下</p>
<p><img src="/" class="lazyload" data-src="C:%5CUsers%5Czgq%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1577713991928.png"  alt="1577713991928"></p>
<p>读数据流程大致如下：</p>
<p><img src="/" class="lazyload" data-src="C:%5CUsers%5Czgq%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1577714051117.png"  alt="1577714051117"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1）客户端通过Distributed FileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址。</span><br><span class="line">2）挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据。</span><br><span class="line">3）DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet为单位来做校验）。</span><br><span class="line">4）客户端以Packet为单位接收，先在本地缓存，然后写入目标文件。</span><br></pre></td></tr></table></figure>



<h3 id="nameNode和SecondNameNode工作原理"><a href="#nameNode和SecondNameNode工作原理" class="headerlink" title="nameNode和SecondNameNode工作原理"></a>nameNode和SecondNameNode工作原理</h3><p>nameNode1工作原理：</p>
<p>nameNode在工作时，数据是存储在内存中的，只要是存在内存中的重要数据，就会有持久化的问题。NameNode的数据持久方式类似于redis的数据持久化方式:类AOF和RDB类(AOF:每次redis执行一条命令就存下来。RDB:每个一段时间，存下数据的快照)</p>
<p>在nameNode中，使用了两种方案结合的持久化方式 。FsImage和Edits。fsImage是nameNode中数据截止到某一时刻的快照。但是，如何nameNode内存中一部分数据还没来得及生成快照就宕机了，那内存中的这部分数据就丢失了。所以引入了Edits。每当生成一份快照之后，就会将开始生成快照这一刻的所有到nameNode的命令都记录在Edits中，这样，fsImage和Edits合并一下，就是NameNode中的完整数据了。</p>
<p>具体流程如下：</p>
<p><img src="/" class="lazyload" data-src="C:%5CUsers%5Czgq%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1577715253715.png"  alt="1577715253715"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">NameNode启动</span><br><span class="line">（1）第一次启动NameNode格式化后，创建Fsimage和Edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。</span><br><span class="line">（2）客户端对元数据进行增删改的请求。</span><br><span class="line">（3）NameNode记录操作日志，更新滚动日志。</span><br><span class="line">（4）NameNode在内存中对元数据进行增删改。</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Secondary NameNode工作</span><br><span class="line">	（1）Secondary NameNode询问NameNode是否需要CheckPoint。直接带回NameNode是否检查结果。</span><br><span class="line">	（2）Secondary NameNode请求执行CheckPoint。</span><br><span class="line">	（3）NameNode滚动正在写的Edits日志。</span><br><span class="line">	（4）将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode。</span><br><span class="line">	（5）Secondary NameNode加载编辑日志和镜像文件到内存，并合并。</span><br><span class="line">	（6）生成新的镜像文件fsimage.chkpoint。</span><br><span class="line">	（7）拷贝fsimage.chkpoint到NameNode。</span><br><span class="line">	（8）NameNode将fsimage.chkpoint重新命名成fsimage。</span><br></pre></td></tr></table></figure>

<p>这里有个文字说明</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Fsimage：NameNode内存中元数据序列化后形成的文件。</span><br><span class="line">Edits：记录客户端更新元数据信息的每一步操作（可通过Edits运算出元数据）。</span><br><span class="line">NameNode启动时，先滚动Edits并生成一个空的edits.inprogress，然后加载Edits和Fsimage到内存中，此时NameNode内存就持有最新的元数据信息。Client开始对NameNode发送元数据的增删改的请求，这些请求的操作首先会被记录到edits.inprogress中（查询元数据的操作不会被记录在Edits中，因为查询操作不会更改元数据信息），如果此时NameNode挂掉，重启后会从Edits中读取元数据的信息。然后，NameNode会在内存中执行元数据的增删改的操作。</span><br><span class="line">由于Edits中记录的操作会越来越多，Edits文件会越来越大，导致NameNode在启动加载Edits时会很慢，所以需要对Edits和Fsimage进行合并（所谓合并，就是将Edits和Fsimage加载到内存中，照着Edits中的操作一步步执行，最终形成新的Fsimage）。SecondaryNameNode的作用就是帮助NameNode进行Edits和Fsimage的合并工作。</span><br><span class="line">SecondaryNameNode首先会询问NameNode是否需要CheckPoint（触发CheckPoint需要满足两个条件中的任意一个，定时时间到和Edits中数据写满了）。直接带回NameNode是否检查结果。SecondaryNameNode执行CheckPoint操作，首先会让NameNode滚动Edits并生成一个空的edits.inprogress，滚动Edits的目的是给Edits打个标记，以后所有新的操作都写入edits.inprogress，其他未合并的Edits和Fsimage会拷贝到SecondaryNameNode的本地，然后将拷贝的Edits和Fsimage加载到内存中进行合并，生成fsimage.chkpoint，然后将fsimage.chkpoint拷贝给NameNode，重命名为Fsimage后替换掉原来的Fsimage。NameNode在启动时就只需要加载之前未合并的Edits和Fsimage即可，因为合并过的Edits中的元数据信息已经被记录在Fsimage中。</span><br></pre></td></tr></table></figure>

<h3 id="fsImage和Edits解析"><a href="#fsImage和Edits解析" class="headerlink" title="fsImage和Edits解析"></a>fsImage和Edits解析</h3><p><img src="/" class="lazyload" data-src="C:%5CUsers%5Czgq%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1577716131261.png"  alt="1577716131261"></p>
<h4 id="ovi命令和oev命令"><a href="#ovi命令和oev命令" class="headerlink" title="ovi命令和oev命令"></a>ovi命令和oev命令</h4><p>ovi：将fsImage转换为其他文件类型</p>
<p>oev：将edits文件转换为其他文件</p>
<p>ovi命令使用</p>
<p>hdfs oiv -p 文件类型 -i  镜像名 -o 转换的文件输出地址</p>
<p>hdfs oev -p 文件类型 -i 编辑日志 -o 转换后文件输出路径。</p>
<h3 id="checkPoint时间设置："><a href="#checkPoint时间设置：" class="headerlink" title="checkPoint时间设置："></a>checkPoint时间设置：</h3><p>前面说到SecondNameNode进行CheckPoint的条件有2个。第一：CheckPoint的间隔时间到了(默认1小时)，或者Edits中记录满了(记录的操作达到100W)。</p>
<p>这些都是可自定义配置的。</p>
<p>hdfs-site.xml文件中配置</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.txns<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1000000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>操作动作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.check.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>60<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span> 1分钟检查一次操作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span> &gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="nameNode故障处理"><a href="#nameNode故障处理" class="headerlink" title="nameNode故障处理"></a>nameNode故障处理</h3><p>第一种：</p>
<p>将SecondaryNameNode中数据拷贝到NameNode存储数据的目录</p>
<p>先杀掉nameNode，删除nameNode的数据，然后把SecondNameNode节点的数据复制到NameNode数据目录下。重新启动NameNode</p>
<p>第二种</p>
<p>使用-importCheckpoint选项启动NameNode守护进程，从而将SecondaryNameNode中数据拷贝到NameNode目录中</p>
<p>1、需要在hdfs-site.xml文件中配置nameNode数据的目录</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.namenode.name.dir&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;&#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2&#x2F;data&#x2F;tmp&#x2F;dfs&#x2F;name&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>

<p>杀掉NameNode进程，删除NameNode存储的数据。</p>
<p>如果SecondNameNode和NameNode不在同一节点，需要手动将SecondNameNode数据复制到NameNode的数据目录下，并删除in_use.lock文件。在同一节点就不用拷贝，要不要删除in_use.lock文件待定(TBD)</p>
<p>导出检查点数据(等待一会ctrl +C结束)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs nameNode -importCheckpoint</span><br></pre></td></tr></table></figure>

<p>启动nameNode</p>
<h2 id="集群安全模式"><a href="#集群安全模式" class="headerlink" title="集群安全模式"></a>集群安全模式</h2><p><img src="/" class="lazyload" data-src="C:%5CUsers%5Czgq%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1577719240493.png"  alt="1577719240493"></p>
<p>集群处于安全模式，不能执行重要操作（写操作）。集群启动完成后，自动退出安全模式。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1、查看安全模式 </span><br><span class="line">hdfs dfsadmin -safemode get</span><br><span class="line">2、进入安全模式</span><br><span class="line">hdfs dfsadmin -safemode enter</span><br><span class="line">3、离开安全模式</span><br><span class="line">hdfs dfsadmin -safemode leave</span><br><span class="line">4、等待安全模式</span><br><span class="line">hdfs dfsadmin -safemode wait</span><br></pre></td></tr></table></figure>



<h2 id="DataNode"><a href="#DataNode" class="headerlink" title="DataNode"></a>DataNode</h2><h3 id="工作机制"><a href="#工作机制" class="headerlink" title="工作机制"></a>工作机制</h3><p><img src="/" class="lazyload" data-src="C:%5CUsers%5Czgq%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1577719552716.png"  alt="1577719552716"></p>
<p>一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳。</p>
<p>2）DataNode启动后向NameNode注册，通过后，周期性（1小时）的向NameNode上报所有的块信息。</p>
<p>3）心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟没有收到某个DataNode的心跳，则认为该节点不可用。</p>
<p>4）集群运行中可以安全加入和退出一些机器。</p>
<h3 id="nameNode掉线时间参数设置"><a href="#nameNode掉线时间参数设置" class="headerlink" title="nameNode掉线时间参数设置"></a>nameNode掉线时间参数设置</h3><p><img src="/" class="lazyload" data-src="C:%5CUsers%5Czgq%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1577719723283.png"  alt="1577719723283"></p>
<p>该参数在hdfs-site.xml文件中设置，有2个参数，recheck-interval单位是毫秒，默认30秒，interval单位秒，默认是600秒</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.heartbeat.recheck-interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>300000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.heartbeat.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>600<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="新增节点和退役节点"><a href="#新增节点和退役节点" class="headerlink" title="新增节点和退役节点"></a>新增节点和退役节点</h3><p>前面讲到，hadoop的集群是很容易进行扩展的。</p>
<h4 id="新增节点"><a href="#新增节点" class="headerlink" title="新增节点"></a>新增节点</h4><p>新增节点比较简单，由于是虚拟机环境，所以就直接复制一台虚拟机了。实际情况是要重新配置一台新的机器的。</p>
<p>单节点启动datanode和nodeManager就可以了。该节点就会自动加入集群中。</p>
<p>如果新加入的节点出现数据不均衡，可以用命令实现集群的再平衡</p>
<p>执行该脚本即可：start-balancer.sh</p>
<h4 id="退役节点："><a href="#退役节点：" class="headerlink" title="退役节点："></a>退役节点：</h4><p>退役旧节点，hadoop提供了2中方案，白名单方案和黑名单方案。</p>
<h5 id="白名单方案："><a href="#白名单方案：" class="headerlink" title="白名单方案："></a>白名单方案：</h5><p>只有添加到白名单的主机节点才允许访问namenode，其他主机都会被移出集群。</p>
<p>配置方式如下：</p>
<p>首先添加一个白名单的主机列表文件，文件名自定义。</p>
<p>然后在hdfs-site.xml中加入下配置，指定该集群的namenode只能由这几台主机访问</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/etc/hadoop/hdfs.hosts<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>将配置文件分发到每个节点上，然后刷新一下nameNode，更新ResourceManager</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -refreshNodes #刷新nameNode</span><br><span class="line">yarn rmadmin -refreshNodes	#刷新ResourceManager</span><br></pre></td></tr></table></figure>

<p>这样，不在该文件中的主机就被移出集群了，在集群的node中是不可见的，并且会自动关闭移除主机的datanode。</p>
<p>同样，如果有数据不均衡问题，可以执行该脚本</p>
<p>start-balancer.sh</p>
<h5 id="黑名单方案"><a href="#黑名单方案" class="headerlink" title="黑名单方案"></a>黑名单方案</h5><p>和白名单有所不同，黑名单是只会移除加入黑名单的节点。并且，使用黑名单移除的节点，在集群节点中还是可见的，只是没有存储数据。</p>
<p>配置方式如下</p>
<p>同样，添加黑名单注解列表文件</p>
<p>然后在hdfs-site.xml中加入dfs.hosts.exclude的配置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.hosts.exclude&lt;&#x2F;name&gt;</span><br><span class="line">      &lt;value&gt;&#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2&#x2F;etc&#x2F;hadoop&#x2F;hdfs.hosts.exclude&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>

<p>然后刷新namenode和ResourceManager。</p>
<p>可以看到退役的节点有一个decommission in progress的状态。这个状态是该节点在复制数据到其他节点上。要注意的是，如何数据副本数设置为3，现在只有3各节点，在这时候退役节点是不会成功的。需要修改副本数才能退役成功。</p>
<p>同样，如果有数据不均衡问题，可以执行该脚本</p>
<p>start-balancer.sh</p>
<h4 id="dataNode多目录设置"><a href="#dataNode多目录设置" class="headerlink" title="dataNode多目录设置"></a>dataNode多目录设置</h4><p>dataNode的数据目录可以配置成多个目录，数据就会分散存储在配置的多个目录中，这几个目录中的数据合起来才是该dataNodde的全部数据。配置如下，在hdfs-site.xml中加入下配置</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///$&#123;hadoop.tmp.dir&#125;/dfs/data1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/data2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>多个目录间用逗号分开。这个配置在扩节点磁盘的时候很有用。可以通过该配置将datanode的数据存储到新磁盘。</p>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">zgq</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://avajbuhtig.github.io/2019/12/29/hadoop%E4%B9%8Bhdfs/">https://avajbuhtig.github.io/2019/12/29/hadoop%E4%B9%8Bhdfs/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://avajbuhtig.github.io" target="_blank">zgq's blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="/img/post.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"/><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/2020/01/01/hadoop%E4%B9%8B%E5%9F%BA%E7%A1%80%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"><img class="prev_cover lazyload" data-src="/img/post.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">hadoop之基础集群搭建</div></div></a></div><div class="next-post pull_right"><a href="/2019/12/05/hadoop%E5%9F%BA%E6%9C%AC/"><img class="next_cover lazyload" data-src="/img/post.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">hadoop基本</div></div></a></div></nav></article></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2020 By zgq</div><div class="framework-info"><span>驱动 </span><a href="https://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">繁</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script></body></html>